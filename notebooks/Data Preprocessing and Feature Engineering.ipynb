{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "591c9a0b",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Feature Engineering\n",
    "\n",
    "Objective:\n",
    "- Clean and handle missing values\n",
    "- Fix data quality issues identified in EDA\n",
    "- Create new features for modeling\n",
    "- Encode categorical variables\n",
    "- Prepare data for machine learning models\n",
    "\n",
    "Key Issues to Address:\n",
    "- Column name typo: coustomer_key -> customer_key\n",
    "- Missing values: 3,723 in fact_sales.unit, 27 in customer names, 1 in item unit\n",
    "- Date parsing and time-based feature creation\n",
    "- RFM scores and customer segmentation features\n",
    "- Product category encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3229987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "Starting preprocessing for 1,000,000 transactions\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data_path = 'data/'\n",
    "\n",
    "fact_sales = pd.read_csv(f'{data_path}fact_sales.csv', encoding='latin-1')\n",
    "dim_customer = pd.read_csv(f'{data_path}dim_customer.csv', encoding='latin-1')\n",
    "dim_item = pd.read_csv(f'{data_path}dim_item.csv', encoding='latin-1')\n",
    "dim_store = pd.read_csv(f'{data_path}dim_store.csv', encoding='latin-1')\n",
    "dim_time = pd.read_csv(f'{data_path}dim_time.csv', encoding='latin-1')\n",
    "dim_payment = pd.read_csv(f'{data_path}dim_payment.csv', encoding='latin-1')\n",
    "\n",
    "print(\"Data loaded successfully\")\n",
    "print(f\"Starting preprocessing for {fact_sales.shape[0]:,} transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db167da",
   "metadata": {},
   "source": [
    "## Data Quality Fixes\n",
    "\n",
    "Addressing column naming issues and missing values identified during EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc89a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIXING COLUMN NAME ISSUES:\n",
      "================================================================================\n",
      "\n",
      "Before:\n",
      "Fact Sales columns with 'coustomer': ['coustomer_key']\n",
      "Customer dim columns with 'coustomer': ['coustomer_key']\n",
      "\n",
      "After:\n",
      "Fact Sales columns: ['payment_key', 'customer_key', 'time_key', 'item_key', 'store_key', 'quantity', 'unit', 'unit_price', 'total_price']\n",
      "Customer dim columns: ['customer_key', 'name', 'contact_no', 'nid']\n",
      "\n",
      "Column name corrections applied successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"FIXING COLUMN NAME ISSUES:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nBefore:\")\n",
    "print(f\"Fact Sales columns with 'coustomer': {[col for col in fact_sales.columns if 'coustomer' in col.lower()]}\")\n",
    "print(f\"Customer dim columns with 'coustomer': {[col for col in dim_customer.columns if 'coustomer' in col.lower()]}\")\n",
    "\n",
    "fact_sales.rename(columns={'coustomer_key': 'customer_key'}, inplace=True)\n",
    "dim_customer.rename(columns={'coustomer_key': 'customer_key'}, inplace=True)\n",
    "\n",
    "print(\"\\nAfter:\")\n",
    "print(f\"Fact Sales columns: {fact_sales.columns.tolist()}\")\n",
    "print(f\"Customer dim columns: {dim_customer.columns.tolist()}\")\n",
    "\n",
    "print(\"\\nColumn name corrections applied successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd5fb3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING VALUE ANALYSIS:\n",
      "================================================================================\n",
      "\n",
      "Fact Sales missing values:\n",
      "payment_key        0\n",
      "customer_key       0\n",
      "time_key           0\n",
      "item_key           0\n",
      "store_key          0\n",
      "quantity           0\n",
      "unit            3723\n",
      "unit_price         0\n",
      "total_price        0\n",
      "dtype: int64\n",
      "\n",
      "Customer dimension missing values:\n",
      "customer_key     0\n",
      "name            27\n",
      "contact_no       0\n",
      "nid              0\n",
      "dtype: int64\n",
      "\n",
      "Item dimension missing values:\n",
      "item_key       0\n",
      "item_name      0\n",
      "desc           0\n",
      "unit_price     0\n",
      "man_country    0\n",
      "supplier       0\n",
      "unit           1\n",
      "dtype: int64\n",
      "\n",
      "================================================================================\n",
      "HANDLING MISSING VALUES:\n",
      "================================================================================\n",
      "\n",
      "1. Fact Sales - Unit column (3,723 missing):\n",
      "   After filling from item dimension: 3723 missing\n",
      "   Filled remaining with mode: 'ct'\n",
      "\n",
      "2. Customer dimension - Name column (27 missing):\n",
      "   Filled with 'Unknown'\n",
      "\n",
      "3. Item dimension - Unit column (1 missing):\n",
      "   Filled with mode: 'ct'\n",
      "\n",
      "4. Payment dimension - Bank name (1 missing for cash):\n",
      "   Filled with 'N/A' for cash transactions\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION - Remaining missing values:\n",
      "Fact Sales: 0\n",
      "Customer: 0\n",
      "Item: 0\n",
      "Store: 0\n",
      "Time: 0\n",
      "Payment: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"MISSING VALUE ANALYSIS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nFact Sales missing values:\")\n",
    "print(fact_sales.isnull().sum())\n",
    "\n",
    "print(\"\\nCustomer dimension missing values:\")\n",
    "print(dim_customer.isnull().sum())\n",
    "\n",
    "print(\"\\nItem dimension missing values:\")\n",
    "print(dim_item.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HANDLING MISSING VALUES:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. Fact Sales - Unit column (3,723 missing):\")\n",
    "fact_sales_with_item = fact_sales.merge(dim_item[['item_key', 'unit']], \n",
    "                                         on='item_key', \n",
    "                                         how='left', \n",
    "                                         suffixes=('', '_from_item'))\n",
    "fact_sales['unit'] = fact_sales['unit'].fillna(fact_sales_with_item['unit_from_item'])\n",
    "remaining_missing = fact_sales['unit'].isnull().sum()\n",
    "print(f\"   After filling from item dimension: {remaining_missing} missing\")\n",
    "\n",
    "if remaining_missing > 0:\n",
    "    mode_unit = fact_sales['unit'].mode()[0]\n",
    "    fact_sales['unit'].fillna(mode_unit, inplace=True)\n",
    "    print(f\"   Filled remaining with mode: '{mode_unit}'\")\n",
    "\n",
    "print(f\"\\n2. Customer dimension - Name column (27 missing):\")\n",
    "dim_customer['name'].fillna('Unknown', inplace=True)\n",
    "print(f\"   Filled with 'Unknown'\")\n",
    "\n",
    "print(f\"\\n3. Item dimension - Unit column (1 missing):\")\n",
    "mode_item_unit = dim_item['unit'].mode()[0]\n",
    "dim_item['unit'].fillna(mode_item_unit, inplace=True)\n",
    "print(f\"   Filled with mode: '{mode_item_unit}'\")\n",
    "\n",
    "print(f\"\\n4. Payment dimension - Bank name (1 missing for cash):\")\n",
    "dim_payment['bank_name'].fillna('N/A', inplace=True)\n",
    "print(f\"   Filled with 'N/A' for cash transactions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION - Remaining missing values:\")\n",
    "print(f\"Fact Sales: {fact_sales.isnull().sum().sum()}\")\n",
    "print(f\"Customer: {dim_customer.isnull().sum().sum()}\")\n",
    "print(f\"Item: {dim_item.isnull().sum().sum()}\")\n",
    "print(f\"Store: {dim_store.isnull().sum().sum()}\")\n",
    "print(f\"Time: {dim_time.isnull().sum().sum()}\")\n",
    "print(f\"Payment: {dim_payment.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63152e4",
   "metadata": {},
   "source": [
    "## Master Dataset Creation\n",
    "\n",
    "Creating a comprehensive denormalized dataset with all corrections and joins for feature engineering.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d80f1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master dataset created with all dimensions\n",
      "Shape: 1,000,000 rows x 31 columns\n",
      "\n",
      "Missing values in master dataset: 0\n",
      "\n",
      "Columns: ['payment_key', 'customer_key', 'time_key', 'item_key', 'store_key', 'quantity', 'unit', 'unit_price', 'total_price', 'name', 'contact_no', 'nid', 'item_name', 'desc', 'unit_price_item', 'man_country', 'supplier', 'unit_item', 'division', 'district', 'upazila', 'date', 'hour', 'day', 'week', 'month', 'quarter', 'year', 'trans_type', 'bank_name', 'date_parsed']\n"
     ]
    }
   ],
   "source": [
    "master_df = fact_sales.copy()\n",
    "\n",
    "master_df = master_df.merge(dim_customer, on='customer_key', how='left', suffixes=('', '_cust'))\n",
    "master_df = master_df.merge(dim_item, on='item_key', how='left', suffixes=('', '_item'))\n",
    "master_df = master_df.merge(dim_store, on='store_key', how='left')\n",
    "master_df = master_df.merge(dim_time, on='time_key', how='left')\n",
    "master_df = master_df.merge(dim_payment, on='payment_key', how='left')\n",
    "\n",
    "master_df['date_parsed'] = pd.to_datetime(master_df['date'], format='%d-%m-%Y %H:%M')\n",
    "\n",
    "print(\"Master dataset created with all dimensions\")\n",
    "print(f\"Shape: {master_df.shape[0]:,} rows x {master_df.shape[1]} columns\")\n",
    "print(f\"\\nMissing values in master dataset: {master_df.isnull().sum().sum()}\")\n",
    "print(f\"\\nColumns: {master_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d74c4c",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Creating new features to enhance model performance:\n",
    "- Temporal features (day of week, time of day categories, etc.)\n",
    "- Customer behavior features (RFM scores, purchase patterns)\n",
    "- Product features (price categories, popularity metrics)\n",
    "- Geographic features\n",
    "- Interaction features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d318b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING TEMPORAL FEATURES:\n",
      "================================================================================\n",
      "Temporal features created:\n",
      "- day_of_week, day_name\n",
      "- is_weekend\n",
      "- week_of_year\n",
      "- time_of_day (Morning/Afternoon/Evening/Night)\n",
      "- is_peak_hour\n",
      "- year_month\n",
      "- days_since_start\n",
      "\n",
      "Sample of new temporal features:\n",
      "          date_parsed  day_of_week  day_name  is_weekend time_of_day  \\\n",
      "0 2016-04-26 17:13:00            1   Tuesday           0     Evening   \n",
      "1 2018-06-11 19:08:00            0    Monday           0     Evening   \n",
      "2 2016-11-19 08:19:00            5  Saturday           1     Morning   \n",
      "3 2020-02-01 06:00:00            5  Saturday           1     Morning   \n",
      "4 2014-07-15 02:58:00            1   Tuesday           0       Night   \n",
      "5 2019-06-25 15:36:00            1   Tuesday           0   Afternoon   \n",
      "6 2019-12-17 01:12:00            1   Tuesday           0       Night   \n",
      "7 2017-11-16 07:07:00            3  Thursday           0     Morning   \n",
      "8 2016-10-16 05:49:00            6    Sunday           1       Night   \n",
      "9 2018-02-13 01:12:00            1   Tuesday           0       Night   \n",
      "\n",
      "   is_peak_hour  \n",
      "0             1  \n",
      "1             1  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n",
      "5             0  \n",
      "6             0  \n",
      "7             0  \n",
      "8             0  \n",
      "9             0  \n"
     ]
    }
   ],
   "source": [
    "print(\"CREATING TEMPORAL FEATURES:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "master_df['day_of_week'] = master_df['date_parsed'].dt.dayofweek\n",
    "master_df['day_name'] = master_df['date_parsed'].dt.day_name()\n",
    "master_df['is_weekend'] = master_df['day_of_week'].isin([5, 6]).astype(int)\n",
    "master_df['week_of_year'] = master_df['date_parsed'].dt.isocalendar().week\n",
    "\n",
    "def categorize_time_of_day(hour):\n",
    "    if 6 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "master_df['time_of_day'] = master_df['hour'].apply(categorize_time_of_day)\n",
    "\n",
    "master_df['is_peak_hour'] = master_df['hour'].isin([9, 13, 14, 17, 19]).astype(int)\n",
    "\n",
    "master_df['year_month'] = master_df['date_parsed'].dt.to_period('M')\n",
    "master_df['days_since_start'] = (master_df['date_parsed'] - master_df['date_parsed'].min()).dt.days\n",
    "\n",
    "print(\"Temporal features created:\")\n",
    "print(\"- day_of_week, day_name\")\n",
    "print(\"- is_weekend\")\n",
    "print(\"- week_of_year\")\n",
    "print(\"- time_of_day (Morning/Afternoon/Evening/Night)\")\n",
    "print(\"- is_peak_hour\")\n",
    "print(\"- year_month\")\n",
    "print(\"- days_since_start\")\n",
    "\n",
    "print(f\"\\nSample of new temporal features:\")\n",
    "print(master_df[['date_parsed', 'day_of_week', 'day_name', 'is_weekend', \n",
    "                 'time_of_day', 'is_peak_hour']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1823a2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING CUSTOMER BEHAVIOR FEATURES:\n",
      "================================================================================\n",
      "Customer behavior features created:\n",
      "- recency (days since last purchase)\n",
      "- frequency (total purchases)\n",
      "- monetary_avg (average order value)\n",
      "- rfm_score (combined RFM score)\n",
      "- customer_segment (Champions/Loyal/Potential/At Risk/Lost)\n",
      "- unique_items (product diversity)\n",
      "- unique_stores (store diversity)\n",
      "\n",
      "Customer segment distribution in master dataset:\n",
      "customer_segment\n",
      "Loyal Customers        281267\n",
      "Potential Loyalists    263579\n",
      "Champions              259003\n",
      "Lost Customers         108412\n",
      "At Risk                 87739\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"CREATING CUSTOMER BEHAVIOR FEATURES:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "reference_date = master_df['date_parsed'].max()\n",
    "\n",
    "customer_features = master_df.groupby('customer_key').agg({\n",
    "    'date_parsed': lambda x: (reference_date - x.max()).days,\n",
    "    'payment_key': 'count',\n",
    "    'total_price': ['sum', 'mean', 'std'],\n",
    "    'quantity': ['sum', 'mean'],\n",
    "    'item_key': 'nunique',\n",
    "    'store_key': 'nunique',\n",
    "    'division': lambda x: x.mode()[0] if len(x) > 0 else 'Unknown'\n",
    "}).reset_index()\n",
    "\n",
    "customer_features.columns = ['customer_key', 'recency', 'frequency', 'monetary_total', \n",
    "                            'monetary_avg', 'monetary_std', 'total_quantity', \n",
    "                            'avg_quantity', 'unique_items', 'unique_stores', \n",
    "                            'preferred_division']\n",
    "\n",
    "customer_features['monetary_std'].fillna(0, inplace=True)\n",
    "\n",
    "customer_features['r_score'] = pd.qcut(customer_features['recency'], q=4, \n",
    "                                        labels=[4, 3, 2, 1], duplicates='drop')\n",
    "customer_features['f_score'] = pd.qcut(customer_features['frequency'].rank(method='first'), \n",
    "                                        q=4, labels=[1, 2, 3, 4], duplicates='drop')\n",
    "customer_features['m_score'] = pd.qcut(customer_features['monetary_total'], q=4, \n",
    "                                        labels=[1, 2, 3, 4], duplicates='drop')\n",
    "\n",
    "customer_features['rfm_score'] = (customer_features['r_score'].astype(int) + \n",
    "                                  customer_features['f_score'].astype(int) + \n",
    "                                  customer_features['m_score'].astype(int))\n",
    "\n",
    "def segment_customer(row):\n",
    "    if row['rfm_score'] >= 10:\n",
    "        return 'Champions'\n",
    "    elif row['rfm_score'] >= 8:\n",
    "        return 'Loyal Customers'\n",
    "    elif row['rfm_score'] >= 6:\n",
    "        return 'Potential Loyalists'\n",
    "    elif row['rfm_score'] >= 5:\n",
    "        return 'At Risk'\n",
    "    else:\n",
    "        return 'Lost Customers'\n",
    "\n",
    "customer_features['customer_segment'] = customer_features.apply(segment_customer, axis=1)\n",
    "\n",
    "master_df = master_df.merge(customer_features[['customer_key', 'recency', 'frequency', \n",
    "                                               'monetary_avg', 'rfm_score', 'customer_segment',\n",
    "                                               'unique_items', 'unique_stores']], \n",
    "                            on='customer_key', how='left')\n",
    "\n",
    "print(\"Customer behavior features created:\")\n",
    "print(\"- recency (days since last purchase)\")\n",
    "print(\"- frequency (total purchases)\")\n",
    "print(\"- monetary_avg (average order value)\")\n",
    "print(\"- rfm_score (combined RFM score)\")\n",
    "print(\"- customer_segment (Champions/Loyal/Potential/At Risk/Lost)\")\n",
    "print(\"- unique_items (product diversity)\")\n",
    "print(\"- unique_stores (store diversity)\")\n",
    "\n",
    "print(f\"\\nCustomer segment distribution in master dataset:\")\n",
    "print(master_df['customer_segment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b20b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING PRODUCT FEATURES:\n",
      "================================================================================\n",
      "Product features created:\n",
      "- product_transaction_count (popularity)\n",
      "- product_total_revenue\n",
      "- product_popularity_rank\n",
      "- price_category (Budget/Standard/Premium/Luxury)\n",
      "- discount_indicator\n",
      "- price_vs_category_avg (relative pricing)\n",
      "- revenue_per_unit\n",
      "\n",
      "Price category distribution:\n",
      "price_category\n",
      "Standard    616996\n",
      "Premium     178278\n",
      "Budget      128903\n",
      "Luxury       75823\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample of product features:\n",
      "                            item_name  unit_price price_category  \\\n",
      "0             M&M Peanut Candy 1.7 oz       35.00         Luxury   \n",
      "1           Charmin Ultra Bath Tissue       26.00        Premium   \n",
      "2       Dole Fruit in Gel Cups 4.3 oz       12.50       Standard   \n",
      "3      Paper Bowls 20 oz Ultra Strong       14.00       Standard   \n",
      "4  Waterloo Sparkling Watermelon 12oz        8.00         Budget   \n",
      "5    Premier Protein Shake Choc. 11oz       22.00        Premium   \n",
      "6   Brisk Lemon Iced Tea - 12 oz cans       15.50       Standard   \n",
      "7   Belvita Protein Oats Soft Biscuit       14.00       Standard   \n",
      "8    Fresca Black Cherry - 12 oz cans        6.75         Budget   \n",
      "9              Clear Plastic Cups 9oz       15.00       Standard   \n",
      "\n",
      "   product_popularity_rank  discount_indicator  \n",
      "0                      137                   0  \n",
      "1                       63                   0  \n",
      "2                      149                   0  \n",
      "3                       90                   0  \n",
      "4                       71                   0  \n",
      "5                      111                   0  \n",
      "6                       88                   0  \n",
      "7                       93                   0  \n",
      "8                       36                   0  \n",
      "9                       21                   0  \n"
     ]
    }
   ],
   "source": [
    "print(\"CREATING PRODUCT FEATURES:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "product_popularity = master_df.groupby('item_key').agg({\n",
    "    'payment_key': 'count',\n",
    "    'total_price': 'sum',\n",
    "    'quantity': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "product_popularity.columns = ['item_key', 'product_transaction_count', \n",
    "                              'product_total_revenue', 'product_total_units']\n",
    "\n",
    "product_popularity['product_popularity_rank'] = product_popularity['product_transaction_count'].rank(\n",
    "    ascending=False, method='dense'\n",
    ").astype(int)\n",
    "\n",
    "master_df = master_df.merge(product_popularity, on='item_key', how='left')\n",
    "\n",
    "def categorize_price(price):\n",
    "    if price < 10:\n",
    "        return 'Budget'\n",
    "    elif price < 20:\n",
    "        return 'Standard'\n",
    "    elif price < 30:\n",
    "        return 'Premium'\n",
    "    else:\n",
    "        return 'Luxury'\n",
    "\n",
    "master_df['price_category'] = master_df['unit_price'].apply(categorize_price)\n",
    "\n",
    "master_df['discount_indicator'] = (master_df['unit_price'] < master_df['unit_price_item']).astype(int)\n",
    "\n",
    "category_avg_price = master_df.groupby('desc')['unit_price'].mean()\n",
    "master_df['category_avg_price'] = master_df['desc'].map(category_avg_price)\n",
    "master_df['price_vs_category_avg'] = master_df['unit_price'] / master_df['category_avg_price']\n",
    "\n",
    "master_df['revenue_per_unit'] = master_df['total_price'] / master_df['quantity']\n",
    "\n",
    "print(\"Product features created:\")\n",
    "print(\"- product_transaction_count (popularity)\")\n",
    "print(\"- product_total_revenue\")\n",
    "print(\"- product_popularity_rank\")\n",
    "print(\"- price_category (Budget/Standard/Premium/Luxury)\")\n",
    "print(\"- discount_indicator\")\n",
    "print(\"- price_vs_category_avg (relative pricing)\")\n",
    "print(\"- revenue_per_unit\")\n",
    "\n",
    "print(f\"\\nPrice category distribution:\")\n",
    "print(master_df['price_category'].value_counts())\n",
    "\n",
    "print(f\"\\nSample of product features:\")\n",
    "print(master_df[['item_name', 'unit_price', 'price_category', \n",
    "                 'product_popularity_rank', 'discount_indicator']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c0c029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING GEOGRAPHIC AND INTERACTION FEATURES:\n",
      "================================================================================\n",
      "Geographic features created:\n",
      "- store_total_revenue, store_transaction_count\n",
      "- division_avg_transaction\n",
      "- is_dhaka, is_top_division\n",
      "\n",
      "Interaction features created:\n",
      "- customer_product_interaction\n",
      "- customer_spending_velocity\n",
      "- product_quantity_ratio\n",
      "\n",
      "Dhaka vs Other divisions:\n",
      "is_dhaka\n",
      "0    613112\n",
      "1    386888\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Current master dataset shape: (1000000, 65)\n",
      "Total features: 65\n"
     ]
    }
   ],
   "source": [
    "print(\"CREATING GEOGRAPHIC AND INTERACTION FEATURES:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "store_performance = master_df.groupby('store_key').agg({\n",
    "    'total_price': ['sum', 'count', 'mean']\n",
    "}).reset_index()\n",
    "store_performance.columns = ['store_key', 'store_total_revenue', \n",
    "                             'store_transaction_count', 'store_avg_transaction']\n",
    "\n",
    "master_df = master_df.merge(store_performance, on='store_key', how='left')\n",
    "\n",
    "division_performance = master_df.groupby('division').agg({\n",
    "    'total_price': 'mean',\n",
    "    'payment_key': 'count'\n",
    "}).reset_index()\n",
    "division_performance.columns = ['division', 'division_avg_transaction', 'division_transaction_count']\n",
    "\n",
    "master_df = master_df.merge(division_performance, on='division', how='left')\n",
    "\n",
    "master_df['is_dhaka'] = (master_df['division'] == 'DHAKA').astype(int)\n",
    "master_df['is_top_division'] = master_df['division'].isin(['DHAKA', 'CHITTAGONG', 'RAJSHAHI']).astype(int)\n",
    "\n",
    "master_df['customer_product_interaction'] = master_df['frequency'] * master_df['product_transaction_count']\n",
    "master_df['customer_spending_velocity'] = master_df['monetary_avg'] / (master_df['recency'] + 1)\n",
    "master_df['product_quantity_ratio'] = master_df['quantity'] / master_df['product_total_units']\n",
    "\n",
    "print(\"Geographic features created:\")\n",
    "print(\"- store_total_revenue, store_transaction_count\")\n",
    "print(\"- division_avg_transaction\")\n",
    "print(\"- is_dhaka, is_top_division\")\n",
    "\n",
    "print(\"\\nInteraction features created:\")\n",
    "print(\"- customer_product_interaction\")\n",
    "print(\"- customer_spending_velocity\")\n",
    "print(\"- product_quantity_ratio\")\n",
    "\n",
    "print(f\"\\nDhaka vs Other divisions:\")\n",
    "print(master_df['is_dhaka'].value_counts())\n",
    "\n",
    "print(f\"\\nCurrent master dataset shape: {master_df.shape}\")\n",
    "print(f\"Total features: {master_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d3e670",
   "metadata": {},
   "source": [
    "## Categorical Variable Encoding\n",
    "\n",
    "Preparing categorical variables for machine learning models through label encoding and one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bae6be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODING CATEGORICAL VARIABLES:\n",
      "================================================================================\n",
      "- desc: 30 unique values encoded\n",
      "- division: 7 unique values encoded\n",
      "- district: 64 unique values encoded\n",
      "- quarter: 4 unique values encoded\n",
      "- trans_type: 3 unique values encoded\n",
      "- time_of_day: 4 unique values encoded\n",
      "- day_name: 7 unique values encoded\n",
      "- customer_segment: 5 unique values encoded\n",
      "- price_category: 4 unique values encoded\n",
      "\n",
      "================================================================================\n",
      "CREATING ONE-HOT ENCODED FEATURES FOR KEY CATEGORIES:\n",
      "\n",
      "One-hot encoded columns added:\n",
      "- Payment types: ['payment_card', 'payment_cash', 'payment_mobile']\n",
      "- Time of day: ['time_Afternoon', 'time_Evening', 'time_Morning', 'time_Night']\n",
      "- Customer segments: ['segment_At Risk', 'segment_Champions', 'segment_Lost Customers', 'segment_Loyal Customers', 'segment_Potential Loyalists']\n",
      "\n",
      "Final master dataset shape: (1000000, 86)\n",
      "Total columns: 86\n"
     ]
    }
   ],
   "source": [
    "print(\"ENCODING CATEGORICAL VARIABLES:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "categorical_cols = ['desc', 'division', 'district', 'quarter', 'trans_type', \n",
    "                   'time_of_day', 'day_name', 'customer_segment', 'price_category']\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in master_df.columns:\n",
    "        le = LabelEncoder()\n",
    "        master_df[f'{col}_encoded'] = le.fit_transform(master_df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"- {col}: {len(le.classes_)} unique values encoded\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING ONE-HOT ENCODED FEATURES FOR KEY CATEGORIES:\")\n",
    "\n",
    "payment_dummies = pd.get_dummies(master_df['trans_type'], prefix='payment')\n",
    "time_dummies = pd.get_dummies(master_df['time_of_day'], prefix='time')\n",
    "segment_dummies = pd.get_dummies(master_df['customer_segment'], prefix='segment')\n",
    "\n",
    "master_df = pd.concat([master_df, payment_dummies, time_dummies, segment_dummies], axis=1)\n",
    "\n",
    "print(f\"\\nOne-hot encoded columns added:\")\n",
    "print(f\"- Payment types: {list(payment_dummies.columns)}\")\n",
    "print(f\"- Time of day: {list(time_dummies.columns)}\")\n",
    "print(f\"- Customer segments: {list(segment_dummies.columns)}\")\n",
    "\n",
    "print(f\"\\nFinal master dataset shape: {master_df.shape}\")\n",
    "print(f\"Total columns: {master_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23453edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING SUMMARY:\n",
      "================================================================================\n",
      "\n",
      "1. DATA QUALITY FIXES:\n",
      "   - Fixed column name: coustomer_key -> customer_key\n",
      "   - Handled all missing values (0 remaining)\n",
      "   - Cleaned and standardized all dimension tables\n",
      "\n",
      "2. FEATURE ENGINEERING SUMMARY:\n",
      "   Original columns: 31\n",
      "   Total columns after engineering: 86\n",
      "   New features created: 55\n",
      "\n",
      "3. FEATURE CATEGORIES:\n",
      "\n",
      "   Temporal Features: 6 features\n",
      "      day_of_week, is_weekend, time_of_day, is_peak_hour, days_since_start...\n",
      "\n",
      "   Customer Features: 7 features\n",
      "      recency, frequency, monetary_avg, rfm_score, customer_segment...\n",
      "\n",
      "   Product Features: 5 features\n",
      "      product_transaction_count, product_popularity_rank, price_category, price_vs_category_avg, discount_indicator\n",
      "\n",
      "   Geographic Features: 4 features\n",
      "      store_total_revenue, division_avg_transaction, is_dhaka, is_top_division\n",
      "\n",
      "   Interaction Features: 3 features\n",
      "      customer_product_interaction, customer_spending_velocity, product_quantity_ratio\n",
      "\n",
      "   Encoded Features: 4 features\n",
      "      desc_encoded, division_encoded, trans_type_encoded, customer_segment_encoded\n",
      "\n",
      "   One-Hot Features: 7 features\n",
      "      payment_card, payment_cash, payment_mobile, time_Morning, time_Afternoon...\n",
      "\n",
      "================================================================================\n",
      "4. DATA QUALITY VERIFICATION:\n",
      "   Total rows: 1,000,000\n",
      "   Total columns: 86\n",
      "   Missing values: 0\n",
      "   Duplicate rows: 0\n",
      "   Memory usage: 1843.52 MB\n",
      "\n",
      "5. NUMERIC FEATURE STATISTICS:\n",
      "   Total numeric features: 48\n",
      "\n",
      "   Sample statistics:\n",
      "          total_price        quantity       frequency       rfm_score  \\\n",
      "count  1000000.000000  1000000.000000  1000000.000000  1000000.000000   \n",
      "mean       105.401436        6.000185      109.796310        7.716562   \n",
      "std         80.829301        3.161932       10.409702        2.396878   \n",
      "min          6.000000        1.000000       73.000000        3.000000   \n",
      "25%         47.250000        3.000000      103.000000        6.000000   \n",
      "50%         90.000000        6.000000      110.000000        8.000000   \n",
      "75%        144.000000        9.000000      117.000000       10.000000   \n",
      "max        605.000000       11.000000      156.000000       12.000000   \n",
      "\n",
      "       product_popularity_rank  \n",
      "count           1000000.000000  \n",
      "mean                 82.728808  \n",
      "std                  42.950252  \n",
      "min                   1.000000  \n",
      "25%                  49.000000  \n",
      "50%                  83.000000  \n",
      "75%                 116.000000  \n",
      "max                 163.000000  \n"
     ]
    }
   ],
   "source": [
    "print(\"PREPROCESSING SUMMARY:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATA QUALITY FIXES:\")\n",
    "print(\"   - Fixed column name: coustomer_key -> customer_key\")\n",
    "print(\"   - Handled all missing values (0 remaining)\")\n",
    "print(\"   - Cleaned and standardized all dimension tables\")\n",
    "\n",
    "print(\"\\n2. FEATURE ENGINEERING SUMMARY:\")\n",
    "print(\"   Original columns: 31\")\n",
    "print(\"   Total columns after engineering: 86\")\n",
    "print(\"   New features created: 55\")\n",
    "\n",
    "print(\"\\n3. FEATURE CATEGORIES:\")\n",
    "feature_categories = {\n",
    "    'Temporal Features': ['day_of_week', 'is_weekend', 'time_of_day', 'is_peak_hour', \n",
    "                         'days_since_start', 'week_of_year'],\n",
    "    'Customer Features': ['recency', 'frequency', 'monetary_avg', 'rfm_score', \n",
    "                         'customer_segment', 'unique_items', 'unique_stores'],\n",
    "    'Product Features': ['product_transaction_count', 'product_popularity_rank', \n",
    "                        'price_category', 'price_vs_category_avg', 'discount_indicator'],\n",
    "    'Geographic Features': ['store_total_revenue', 'division_avg_transaction', \n",
    "                           'is_dhaka', 'is_top_division'],\n",
    "    'Interaction Features': ['customer_product_interaction', 'customer_spending_velocity', \n",
    "                            'product_quantity_ratio'],\n",
    "    'Encoded Features': ['desc_encoded', 'division_encoded', 'trans_type_encoded', \n",
    "                        'customer_segment_encoded'],\n",
    "    'One-Hot Features': ['payment_card', 'payment_cash', 'payment_mobile', \n",
    "                        'time_Morning', 'time_Afternoon', 'time_Evening', 'time_Night']\n",
    "}\n",
    "\n",
    "for category, features in feature_categories.items():\n",
    "    available = [f for f in features if f in master_df.columns]\n",
    "    print(f\"\\n   {category}: {len(available)} features\")\n",
    "    print(f\"      {', '.join(available[:5])}{'...' if len(available) > 5 else ''}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. DATA QUALITY VERIFICATION:\")\n",
    "print(f\"   Total rows: {master_df.shape[0]:,}\")\n",
    "print(f\"   Total columns: {master_df.shape[1]}\")\n",
    "print(f\"   Missing values: {master_df.isnull().sum().sum()}\")\n",
    "print(f\"   Duplicate rows: {master_df.duplicated().sum()}\")\n",
    "print(f\"   Memory usage: {master_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n5. NUMERIC FEATURE STATISTICS:\")\n",
    "numeric_cols = master_df.select_dtypes(include=[np.number]).columns\n",
    "print(f\"   Total numeric features: {len(numeric_cols)}\")\n",
    "print(f\"\\n   Sample statistics:\")\n",
    "print(master_df[['total_price', 'quantity', 'frequency', 'rfm_score', \n",
    "                'product_popularity_rank']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa0df074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING PREPROCESSED DATA:\n",
      "================================================================================\n",
      "1. Master dataset saved: data/processed/master_dataset.csv\n",
      "   Shape: (1000000, 86)\n",
      "\n",
      "2. Clean dimension tables saved\n",
      "\n",
      "3. Feature list saved: data/processed/feature_list.json\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Key Deliverables:\n",
      "1. Master dataset with 86 features (1M rows)\n",
      "2. Clean dimension tables with no missing values\n",
      "3. Feature engineering: temporal, customer, product, geographic, interaction\n",
      "4. Categorical encoding: label encoding + one-hot encoding\n",
      "5. Ready for modeling and analysis\n",
      "\n",
      "Next Steps:\n",
      "- Notebook 03: Demand and Revenue Forecasting\n",
      "- Notebook 04: Pricing and Optimization\n",
      "- Notebook 05: Customer Analytics and Segmentation\n"
     ]
    }
   ],
   "source": [
    "print(\"SAVING PREPROCESSED DATA:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_path = 'data/processed/'\n",
    "import os\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "master_df.to_csv(f'{output_path}master_dataset.csv', index=False)\n",
    "print(f\"1. Master dataset saved: {output_path}master_dataset.csv\")\n",
    "print(f\"   Shape: {master_df.shape}\")\n",
    "\n",
    "fact_sales.to_csv(f'{output_path}fact_sales_clean.csv', index=False)\n",
    "dim_customer.to_csv(f'{output_path}dim_customer_clean.csv', index=False)\n",
    "dim_item.to_csv(f'{output_path}dim_item_clean.csv', index=False)\n",
    "dim_store.to_csv(f'{output_path}dim_store_clean.csv', index=False)\n",
    "dim_time.to_csv(f'{output_path}dim_time_clean.csv', index=False)\n",
    "dim_payment.to_csv(f'{output_path}dim_payment_clean.csv', index=False)\n",
    "print(f\"\\n2. Clean dimension tables saved\")\n",
    "\n",
    "feature_list = {\n",
    "    'all_features': list(master_df.columns),\n",
    "    'numeric_features': list(master_df.select_dtypes(include=[np.number]).columns),\n",
    "    'categorical_features': list(master_df.select_dtypes(include=['object']).columns),\n",
    "    'encoded_features': [col for col in master_df.columns if '_encoded' in col],\n",
    "    'onehot_features': [col for col in master_df.columns if any(prefix in col for prefix in ['payment_', 'time_', 'segment_'])]\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'{output_path}feature_list.json', 'w') as f:\n",
    "    json.dump(feature_list, f, indent=4, default=str)\n",
    "print(f\"\\n3. Feature list saved: {output_path}feature_list.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nKey Deliverables:\")\n",
    "print(\"1. Master dataset with 86 features (1M rows)\")\n",
    "print(\"2. Clean dimension tables with no missing values\")\n",
    "print(\"3. Feature engineering: temporal, customer, product, geographic, interaction\")\n",
    "print(\"4. Categorical encoding: label encoding + one-hot encoding\")\n",
    "print(\"5. Ready for modeling and analysis\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"- Notebook 03: Demand and Revenue Forecasting\")\n",
    "print(\"- Notebook 04: Pricing and Optimization\")\n",
    "print(\"- Notebook 05: Customer Analytics and Segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7bdd90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
